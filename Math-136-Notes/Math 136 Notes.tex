\documentclass[12pt, letterpaper]{article}
\usepackage{amsmath,amssymb,amsthm,enumerate,nicefrac,fancyhdr,graphicx,adjustbox,titlesec}
\DeclareMathOperator{\spn}{span}
\title{Math 136 Notes}
\author{Thomas Liu}
\begin{document}
\maketitle
\tableofcontents

\newpage

% Chapter 1
\section{Chapter 1 Vectors in Euclidean Space ($\mathbb{R}^n$)}
\subsection{Introduction}
Linear algebra is the algebraic theory behind geometric ideas such as
\begin{itemize}
    \item lines
    \item planes
    \item perpendicular/parallel
    \item rotation/reflection/translation
    \item megnitude + direction
    \item[] $\cdots$ and their generalization
\end{itemize}
\subsubsection*{Definition}
Let $n\in\mathbb{N}$
\begin{align*}
    \mathbb{R}^n = \{
    \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix}
    : x_i \in \mathbb{R} \}
\end{align*}
We call $\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$ a vector in $\mathbb{R}^n$ and 
we call $x_i$ the $i^{th}$ component of the vector \\
Example: $\begin{bmatrix} 1 \\ \pi \\ \frac{3}{7} \\ 0 \\ -1 \end{bmatrix} \in \mathbb{R}$
\subsection{Geometric Interpretation}
\begin{enumerate}
    \item[Case 1] let $\vec{v} \in \mathbb{R}^2$, say $\vec{v} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$
    \item[Case 2] let $\vec{v} \in \mathbb{R}^3$, say $\vec{v} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}$
\end{enumerate}
\subsection{Operations}
To do linear algebra we need the following operations: \\
\textbf{Definition}: let $\vec{u} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \vec{v} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \in \mathbb{R}^n$, then 
\begin{enumerate}
    \item addition: $\vec{u} + \vec{v} = \begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 \\ \vdots \\ x_n + y_n \end{bmatrix}$
    \item scalar multiplication: for $c\in\mathbb{R}$, $c\vec{u} = \begin{bmatrix} cx_1 \\ cx_2 \\ \vdots \\ cx_n \end{bmatrix}$
\end{enumerate}
Example: In $\mathbb{R}^2$, $2\begin{bmatrix} -1 \\ 3\end{bmatrix} + \begin{bmatrix} 0 \\ -7 \end{bmatrix} = \begin{bmatrix} -2 \\ 6 \end{bmatrix} + \begin{bmatrix} 0 \\ -7 \end{bmatrix} = \begin{bmatrix} -2 \\ -1\end{bmatrix}$ \\
\textbf{More on Scalar Multiplication}:
\begin{enumerate}
    \item $(c+d)\vec{v} = c\vec{v} + d\vec{v}$
    \item $(cd)\vec{v} = c(d\vec{v})$
    \item $c(\vec{u}+\vec{v}) = c\vec{u} + c\vec{v}$
    \item $0\vec{v} = \vec{0}$
    \item If $c\vec{v} = \vec{0}$, then $c=0$ or $\vec{v} = \vec{0}$
\end{enumerate}
\textbf{Standard Basis} \\
In $\mathbb{R}^n$, let $\vec{e_i}$ be the vector whose $i^{th}$ component is $1$ with all other components
0. The set $\varepsilon = \{\vec{e_1}, \vec{e_2}, \cdots, \vec{e_n}\}$ is called the standard basis of $\mathbb{R}^n$ \\
Note that $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = v_1\vec{e_1} + v_2\vec{e_2} + \cdots + v_n\vec{e_n}$
\subsection{Vector in $\mathbb{C}^n$}
Definition: 
\begin{align*}
    \mathbb{C}^n = \{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\ : x_1 \cdots x_n \in\mathbb{C}\}
\end{align*}
\subsection{Dot Product}
Let $\vec{u} = \begin{bmatrix} u_1 \\ \vdots \\ u_n\end{bmatrix}$, $\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n\end{bmatrix}$ be vectors in $\mathbb{R}^n$ \\
We define the dot product: $\vec{u} \cdot \vec{v} = u_1v_1 + u_2v_2 + \cdots + u_nv_n$
\subsection{Properties of Dot Product}
If $c\in\mathbb{R}$ and $\vec{u}, \vec{v}, \vec{w}$ are vectors in $\mathbb{R}^n$
\begin{enumerate}
    \item $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$
    \item $(\vec{u}+\vec{v})\cdot\vec{w} = \vec{u}\cdot\vec{w} + \vec{v}\cdot\vec{w}$
    \item $(c\vec{u})\cdot\vec{v} = c(\vec{u}\cdot\vec{v})$
    \item $\vec{v}\cdot\vec{v} \geq 0$, with $\vec{v}\cdot\vec{v} = 0$ iff $\vec{v} = \vec{0}$
\end{enumerate}
\subsection{Length}
Length of vector $\vec{v}\in\mathbb{R}^n$ is $||\vec{v}|| = \sqrt{\vec{v}\cdot\vec{v}}$. It is also called \textbf{norm} or \textbf{magnitude}.
If $c\in\mathbb{R}$, $\vec{v}\in\mathbb{R}^n$, then $||c\vec{v}|| = |c|||\vec{v}||$
\subsection{Unit Vector}
$\vec{v}\in\mathbb{R}^n$ is unit vector if $||\vec{v}|| = 1$
\subsection{Normalization}
When $\vec{v}\in\mathbb{R}^n$ is a non-zero vector, we produce a unit vector 
\begin{align*}
    \hat{v} = \dfrac{\vec{v}}{||\vec{v}||}
\end{align*}
in the direction of $\vec{v}$ by scale $\vec{v}$. This process is called normalization
\subsection{Angle}
Let $\vec{u}$ and $\vec{v}$ be non-zero vectors in $\mathbb{R}^n$. The angle $\theta$, in radians $(0\leq\theta\leq\pi)$, 
between $\vec{u}$ and $\vec{v}$ is such that 
\begin{align*}
    \vec{u}\cdot\vec{v} = ||\vec{u}||||\vec{v}||\cos\theta 
\end{align*}
\subsection{Orthogonality}
We say that the two vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ are orthogonal / perpendicular if $\vec{u}\cdot\vec{v} = 0$
\subsection{Projection and Perpendicular}
\subsubsection{Projection}
Let $\vec{v}, \vec{w}\in\mathbb{R}^n$ with $\vec{w}\neq\vec{0}$. The projection of $\vec{v}$ onto $\vec{w}$ is defined by
\begin{align*}
    \text{proj}_{\vec{w}}(\vec{v}) = \frac{(\vec{v}\cdot\vec{w})}{||\vec{w}||^2}\vec{w} = \frac{(\vec{v}\cdot\vec{w})}{\vec{w}\cdot\vec{w}}\vec{w}
\end{align*}
\subsubsection{Perpendicular}
Let $\vec{v}, \vec{w}\in\mathbb{R}^n$ with $\vec{w}\neq\vec{0}$. The perpendicular of $\vec{v}$ onto $\vec{w}$ is defined by
\begin{align*}
    \text{perp}_{\vec{w}}(\vec{v}) = \vec{v} - \text{proj}_{\vec{w}}(\vec{v})
\end{align*}
\textbf{Note}: \\
We have
\begin{align*}
    \text{proj}_{\vec{w}}\vec{v} = (\vec{v}\cdot\hat{w})\hat{w}
\end{align*}    
length of $\vec{w}$ is irrelevant \\
Also
\begin{align*}
    \text{proj}_{\vec{w}}\vec{v} &= \dfrac{||\vec{v}||||\vec{w}||\cos\theta}{||\vec{w}||^2}\vec{w} \\
                                 &= ||\vec{v}||\cos\theta\hat{w}
\end{align*}
$||\vec{v}||\cos\theta$ is the scalar component of $\vec{v}$ over $\vec{w}$
\subsection{Fields}
$\mathbb{R}$ and $\mathbb{C}$
\begin{itemize}
    \item can always add, subtract and multiply and stay in the set
    \item can always divide by a non-zero element and stay in the set
    \item "everything works" (commutative, associative, distributive, inverses exist etc.)
\end{itemize}
\subsection{Cross Product}
\textbf{Definition}: \\
Let $\vec{u} = \begin{bmatrix}
    u_1 \\ u_2 \\ u_3
\end{bmatrix}, \vec{v} = \begin{bmatrix}
    v_1 \\ v_2 \\ v_3
\end{bmatrix} \in\mathbb{R}^3$ \\
The cross product of $\vec{u}$ and $\vec{v}$ is the vecttor in $\mathbb{R}^3$ given by
\begin{align*}
    \vec{u} \times \vec{v} = \begin{bmatrix}
        u_2v_3 - u_3v_2 \\
        -(u_1v_3 - u_3v_1) \\
        u_1v_2 - u_2v_1
    \end{bmatrix}
\end{align*}
\textbf{Properties}: \\
Let $\vec{u},\vec{v}\in\mathbb{R}^3$ and let $\vec{z}=\vec{u}\times\vec{v}$. Then
\begin{enumerate}
    \item $\vec{z}\cdot\vec{u}=0$ and $\vec{z}\cdot\vec{v}=0$
    \item $\vec{v}\times\vec{u}=-\vec{z}=-\vec{u}\times\vec{v}$
    \item If $\vec{u}\neq\vec{0}$ and $\vec{v}\neq\vec{0}$, then $||\vec{u}\times\vec{v}||=||\vec{u}||||\vec{v}||\sin\theta$, where $\theta$ is the angle between $\vec{u}$ and $\vec{v}$
\end{enumerate}
\textbf{More Properties}: \\
Notes: 
\begin{itemize}
    \item only defined for $\mathbb{R}^3$
    \item $||\vec{u}\times\vec{v}||$ gives the area of a parallelogram
\end{itemize}
Linearity of the Cross Product: \\
If $c\in\mathbb{R}$ and $\vec{u},\vec{v},\vec{w}\in\mathbb{R}^3$, then
\begin{enumerate}
    \item $(\vec{u}+\vec{v})\times\vec{w} = (\vec{u}\times\vec{w})+(\vec{v}\times\vec{w})$
    \item $(c\vec{u})\times\vec{v} = c(\vec{u}\times\vec{v})$
    \item $\vec{u}\times(\vec{v}+\vec{w}) = (\vec{u}\times\vec{v}) + (\vec{u}\times\vec{w})$
    \item $\vec{u}\times(c\vec{v}) = c(\vec{u}\times\vec{v})$
\end{enumerate}

% Chapter 2
\section{Chapter 2 - Span, Lines and Planes}
\subsection{Linear Combinations and Spans}
\subsubsection{Linear Combination}
Let $c_1, c_2, \cdots, c_k\in\mathbb{F}$ and let $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$ be vectors in 
$\mathbb{F}^n$. We refer any vector of the form $c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_k\vec{v_k}$ as 
a linear combination of $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$
\subsubsection{Span}
Let span of $\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ to be the set of all linear combinations of 
$\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$. That is, Span\[\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\} = \{c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_k\vec{v_k} : c_1, c_2, \cdots, c_k\in\mathbb{F}\}\]
We refer to $\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ as a spanning set for Span$\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$. 
We also say that Span$\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ is spanned by $\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$
\subsection{Geometry}
\begin{itemize}
    \item span$\{\vec{0}\} = \vec{0}$
    \item For $\vec{v}\neq\vec{0}$, span$\{\vec{0}\}$ is a line through origin with direction $\vec{v}$
    \item For $\vec{v}, \vec{w}\neq\vec{0}$, if $\vec{v}$ and $\vec{w}$ are not parallel, then span$\{\vec{v}, \vec{w}\}$ is a plane
\end{itemize}
\subsection{Parametric Equations of a Line in $\mathbb{R}^2$}
Let $p$, $q$ be fixed real numbers and $q\neq0$. The parametric equations of a line in $\mathbb{R}^2$ through the 
point $(x_1, y_1)$ with slope $\frac{p}{q}$ are 
\[\begin{cases}
    x &= x_1 + qt \\
    y &= y_1 + pt
\end{cases}, t\in\mathbb{R}\]
\subsection{Vectors and Lines}
\subsubsection{Vector Equation of a Line in $\mathbb{R}^2$}
Let $\begin{bmatrix} q \\ p \end{bmatrix}$ be a non-zero vector in $\mathbb{R}^2$. The expression 
\[\vec{l} = \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x_1 \\ y_1 \end{bmatrix} + t\begin{bmatrix} q \\ p \end{bmatrix}, t\in\mathbb{R}\]
is a vector equation of $\mathcal{L}$ in $\mathbb{R}^2$ through $\begin{bmatrix} x_1 \\ y_1 \end{bmatrix}$ with 
direction $\begin{bmatrix} q \\ p \end{bmatrix}$
\subsubsection{Line in $\mathbb{R}^2$}
Let $\vec{u}, \vec{v}\in\mathbb{R}^2$ with $\vec{v}\neq\vec{0}$. We refer to the set of vectors 
\[\mathcal{L} = \{\vec{u} + t\vec{v} : t\in\mathbb{R}\}\]
as a line $\mathcal{L}$ in $\mathbb{R}^2$ through $\vec{u}$ with direction $\vec{v}$
\subsection{Equations in $\mathbb{R}^n$}
\subsubsection{Vector Equation of a Line in $\mathbb{R}^n$}
Let $\vec{u}, \vec{v}\in\mathbb{R}^n$ with $\vec{v}\neq\vec{0}$. The expression 
\[\vec{l} = \vec{u}+t\vec{v}, t\in\mathbb{R}\]
is a vector equation of line $\mathcal{L}$ in $\mathbb{R}^n$ through $\vec{u}$ with direction $\vec{v}$
\subsubsection{Parametric Equations of a Line in $\mathbb{R}^n$}
Let $\vec{u},\vec{v}\in\mathbb{R}^n$ with $\vec{v}\neq\vec{0}$. From above, consider 
\[\vec{l} = \vec{u}+t\vec{v}, t\in\mathbb{R}\]
The parametric equations of the line $\mathcal{L}$ in $\mathbb{R}^n$ through $\vec{u}$ with direction $\vec{v}$ are 
\[\begin{cases}
    l_1 = &u_1 + tv_1 \\
    l_2 = &u_2 + tv_2 \\
    &\vdots \\
    l_n = &u_n + tv_n
\end{cases}, t\in\mathbb{R}\]
\subsection{Lines in $\mathbb{R}^n$}
\subsubsection{Line in $\mathbb{R}^n$}
Let $\vec{u},\vec{v}\in\mathbb{R}^n$ with $\vec{v}\neq\vec{0}$. We refer to the set of vectors 
\[\mathcal{L} = \{\vec{u}+t\vec{v} : t\in\mathbb{R}\}\]
as a line $\mathcal{L}$ in $\mathbb{R}^n$ through $\vec{u}$ with direction $\vec{v}$
\subsubsection*{Notes}
\begin{itemize}
    \item If $l_1$ and $l_2$ are lines with direction $\vec{v_1}$ and $\vec{v_2}$, respectively, we say that they have the same direction if $c\vec{v_1} = \vec{v_2}$ for some non-zero $c\in\mathbb{R}$
    \item Lines through the origin can be described as the span of a vector
\end{itemize}
\subsection{Planes Through the Origin in $\mathbb{R}^n$}
\subsubsection{Plane in $\mathbb{R}^n$ Through the Origin}
Let $\vec{v},\vec{w}$ be non-zero vectors in $\mathbb{R}^n$ with $\vec{v}\neq c\vec{w}$ for any $c\in\mathbb{R}$. Then 
\[\mathcal{P} = \text{Span}\{\vec{v}, \vec{w}\} = \{s\vec{v}+t\vec{w} : s,t\in\mathbb{R}\}\]
is a plane in $\mathbb{R}^n$ through the origin with direction vectors $\vec{v}$ and $\vec{w}$
\subsubsection{Vector Equation of a Plane in $\mathbb{R}^n$ Through the Origin}
Let $\vec{v},\vec{w}$ be non-zero vectors in $\mathbb{R}^n$ with $\vec{v}\neq c\vec{w}$ for any $c\in\mathbb{R}$. The expression 
\[\vec{p} = s\vec{v}+t\vec{w}\]
is a vector equation of the plane in $\mathbb{R}^n$ through the origin with direction vectors $\vec{v}$ and $\vec{w}$
\subsection{Arbitrary Planes in $\mathbb{R}^n$}
\subsubsection{Planes in $\mathbb{R}^n$}
Let $\vec{u}\in\mathbb{R}^n$ and let $\vec{v}$ and $\vec{w}$ be non-zero vectors in $\mathbb{R}^n$ with 
$\vec{v}\neq c\vec{w}$, for any $c\in\mathbb{R}$. Then 
\[\mathcal{P} = \{\vec{u} + s\vec{v} + t\vec{w} : s,t\in\mathbb{R}\}\]
is a plane in $\mathbb{R}^n$ through $\vec{u}$ with direction vectors $\vec{v}$ and $\vec{w}$. We say that $\vec{v}$ and $\vec{w}$ are parallel to $\mathcal{P}$
\subsubsection{Vector Equation of a Plane}
Let $\vec{u}\in\mathbb{R}^n$ and let $\vec{v}$ and $\vec{w}$ be non-zero vectors in $\mathbb{R}^n$ with 
$\vec{v}\neq c\vec{w}$, for any $c\in\mathbb{R}$. Then 
\[\vec{p} = \vec{u}+s\vec{v}+t\vec{w}, s,t\in\mathbb{R}\]
is a vector equation of the plane in $\mathbb{R}^n$ through $\vec{u}$ with direction vectors $\vec{v}$ and $\vec{w}$
\subsection{Planes in $\mathbb{R}^3$}
\subsubsection{Normal Vector}
Let $\vec{v}$ and $\vec{w}$ be non-zero vectors in $\mathbb{R}^3$ with $\vec{v}\neq c\vec{w}$, for any 
$c\in\mathbb{R}$. The vector $\vec{n} = \vec{v}\times\vec{w}$ is referred to as a normal vector to the plane 
with direction vectors $\vec{v}$ and $\vec{w}$
\subsubsection{Normal Form, Scalar Equation of a Plane in $\mathbb{R}^3$}
Let $\mathcal{P}$ be a plane in $\mathbb{R}^3$ with direction vectors $\vec{v}$ and $\vec{w}$ and a 
normal vector $\vec{n} = \begin{bmatrix}
    a \\ b \\ c
\end{bmatrix} \neq\vec{0}$. Let $\vec{u}\in\mathcal{P}$ and $\vec{p} = \begin{bmatrix}
    x \\ y \\ z
\end{bmatrix} \in\mathcal{P}$ where $\vec{p}\neq\vec{u}$. A normal form of $\mathcal{P}$ is given by 
\[\vec{n}\cdot(\vec{p}-\vec{u})=0\]
Expanding this, we arrive at a scalar equation (general form) of $\mathcal{P}$, $ax+by+cz = d$, where $d=\vec{n}\cdot\vec{u}$

% Chapter 3
\section{Chapter 3 - Systems of Linear Equations}
\subsection{Linear Equations}
\subsubsection{Linear Equation, Coefficient, Constant Term}
A linear equation in $n$ variables $x_1, x_2, \cdots, x_n$ is an equation that can be written in the form 
\[a_1x_1 + a_2x_2 + \cdots + a_nx_n = b\]
where $a_1, a_2, \cdots, a_n, b\in\mathbb{F}$ \\
The scalars $a_1, a_2, \cdots, a_n$ are the coefficients of $x_1, x_2, \cdots, x_n$, and $b$ is the constant term 
\subsection{Systems of Linear Equations}
A system of linear equations is a collection of $m$ linear equations in $n$ variables, $x_1, \cdots, x_n$: 
\begin{align*}
    a_{11}x_1 + a_{12}x_2 + &\cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + &\cdots + a_{2n}x_n = b_2 \\
    &\vdots \\
    a_{m1}x_1 + a_{m2}x_2 + &\cdots + a_{mn}x_n = b_m
\end{align*}
Use the convention that $a_{ij}$ is the coefficient of $x_j$ in the $i^{th}$ equation 
\subsection{Solution to the System}
We say that the scalars $y_1, y_2, \cdots, y_n$ in $\mathbb{F}$ solve a system of lienar 
equations if, when we set $x_1=y_1, x_2=y_2, \cdots, x_n=y_n$ in the system, then each of the
equations is satisfied: 
\begin{align*}
    a_{11}y_1 + a_{12}y_2 + &\cdots + a_{1n}y_n = b_1 \\
    a_{21}y_1 + a_{22}y_2 + &\cdots + a_{2n}y_n = b_2 \\
    &\vdots \\
    a_{m1}y_1 + a_{m2}y_2 + &\cdots + a_{mn}y_n = b_m
\end{align*}
We also say that the vector $\vec{y} = \begin{bmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}$ is a solution to the system
\subsection{Important Facts}
\subsubsection{Solution Set}
The set of all solutions to a system of linear equations is called the solution set to the system 
\subsubsection{Theorem: The solution Set to a System of Linear Equations}
The solution set to a system of lienar equations is exactly one of the following: 
\begin{enumerate}
    \item[(a)] empty (no solutions)
    \item[(b)] contains exactly one element (unique solution)
    \item[(c)] contains an infinite number of elements (has one or more parameters)
\end{enumerate}
\subsection{System Terminology}
\subsubsection{Inconsistent and Consistent Systems}
If the solution set to a system of linear equations is empty, we say that the system is inconsistent. \\
If the solution set has a unique solution or infinitely many solutions, we say that the system is consistent 
\subsubsection{Equivalent Systems}
We say that two linear systems are equivalent whenever they have the same solution set
\subsection{Elementary Operations}
Consider a system of $m$ linear equations in $n$ variables. The equations are ordered and labelled from $e_1$ to $e_m$. The following 
three operations are known as elementary operations.
\begin{itemize}
    \item Equation swap (interchange equations $e_i$ and $e_j$): \[e_i \iff e_j\]
    \item Equation scale (replace equation $e_i$ by $m$ times $e_i$, $m\neq 0$) \[e_i \rightarrow me_i, m\in\mathbb{R}\setminus{0}\]
    \item Equation addition (replace $e_j$ by $e_j$ plus a multiple of $e_i$) \[e_j \rightarrow ce_i+e_j\]\[i\neq j, c\in\mathbb{F}\]
\end{itemize}
\subsection{More Important Facts}
\subsubsection{Theorem - Elementary Operations}
If a single elementary operation of any type is performed on a system of linear equations, then the system produced will be equivalent to the original system 
\subsubsection{Trivial Equation}
We refer to the equation $0=0$ as the trivial equation. Any other equation is known as a non-trivial equation
\subsection{Matrix}
An $m\times n$ matrix, $A$, is a rectangular array of scalars with $m$ rows and $n$ columns. The scalar in the $i^{th}$ row and $j^{th}$ 
column is the $(i, j)^{th}$ entry and is denoted $a_{ij}$ or $A_{ij}$. That is 
\[A=\begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}\]
\subsection{Coefficient Matrix}
For a given system of linear equations, 
\begin{align*}
    a_{11}x_1 + a_{12}x_2 + & \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + & \cdots + a_{2n}x_n = b_2 \\
                            & \vdots                   \\
    a_{m1}x_1 + a_{m2}x_2 + & \cdots + a_{mn}x_n = b_m 
\end{align*}
the coefficients matrix, $A$, of the system is the matrix 
\[A=\begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}\]
The entry $a_{ij}$ is the coefficient of the variable $x_j$ in the $i^{th}$ equation 
\subsection{Augmented Matrix}
For a given system of linear equations, 
\begin{align*}
    a_{11}x_1 + a_{12}x_2 + & \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + & \cdots + a_{2n}x_n = b_2 \\
                            & \vdots                   \\
    a_{m1}x_1 + a_{m2}x_2 + & \cdots + a_{mn}x_n = b_m 
\end{align*}
the augmented matrix, $[A|\vec{b}]$, of the system is 
\[[A|\vec{b}]=
\begin{bmatrix}
\begin{array}{cccc|c}
    a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
    a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
    \vdots & \vdots & \vdots & \vdots & \vdots\\
    a_{m1} & a_{m2} & \cdots & a_{mn} & b_m
\end{array}
\end{bmatrix}\]
where $\vec{b}$ is a column whose entries are the constant terms on the right-hand side of the equations 
\subsection{Manipulating Rows}
\subsubsection{Elementary Row Operation (ERO)}
Elementary Row Operations (EROs) are the operations performed on the coefficient and/or augmented matrix 
which correspond to the elementary operations performed on the system of equations 
\[\begin{array}{c|c|c}
    \text{Row swap}        & e_i \leftrightarrow e_j           & R_i \leftrightarrow R_j \\
    \text{Row scale}       & e_i \rightarrow ce_i, c\neq 0     & R_i \rightarrow cR_i, c\neq 0 \\
    \text{Row addition}    & e_i \rightarrow ce_j+e_i, i\neq j & R_i \rightarrow cR_j+R_i, i\neq j
\end{array}\]
\subsubsection{Zero Row}
We refer to a row that has all zero entries as a zero row 
\subsubsection{Row Equivalent}
If a matrix $B$ is obtained from a matrix $A$ by a infinite number of EROs, then we 
say that $B$ is row equivalent to $A$
\subsection{REF}
\subsubsection{Leading Entry, Leading One}
The leftmost non-zero entry in any non-zero row of a matrix is called the leading entry of that row. If the 
leading entry is a 1, that it is called a leading one.
\subsubsection{Row Echelon Form}
We say that a matrix is row echelon form (REF) whenever both of the following two conditions are satisfied: 
\begin{enumerate}
    \item All zero rows occur as the final rows in the matrix 
    \item The leading entry in any non-zero row appears in a column to the right of the columns containing the leading entried of any of the rows above it 
\end{enumerate}
We say that the matrix $R$ is a row echelon form of matrix $A$ to mean that $R$ is in row echelon 
form and that $R$ can be obtained from $A$ by performing a finite number of $EROs$ to $A$
\subsection{RREF}
\subsubsection{Pivot, Pivot Position, Pivot Column, Pivot Row}
If a matrix is in $REF$, then the leading entries are referred to as pivots and their positions in the matrix are called pivot positions. 
Any column that contains a pivot position is called a pivot column. Any row that contains a pivot position is called a pivot row.
\subsubsection{Reduced Row Echelon Form}
We say that a matrix is in reduced row echelon form (RREF) whenever 
all of the following three conditions are satisfied:
\begin{enumerate}
    \item It is in REF
    \item All its pivots are leading ones 
    \item The only non-zero entry in a pivot column is the pivot itself
\end{enumerate}
\subsection*{Important Result}
\subsubsection{Theorem - Unique RREF}
Let $A$ be a matrix with REFs $R_1$ and $R_2$. Then $R_1$ and $R_2$ will have the same set of pivot positions. 
Moreover, there is a unique matrix $R$ such that $R$ is the RREF of A
\subsubsection{RREF(A)}
We say that the matrix $R$ is the reduced row echelon form of matrix A, and we write $R = RREF(A)$, if $R$ 
is in reduced row echelon form and if $R$ can be obtained from $A$ by performing a finite number of EROs to A
\subsection{Obtaining an REF}
\subsubsection*{Algorithm}
\begin{enumerate}
    \item Conside the leftmost non-zero column of the matrix. Use EROs to obtain a leading entry in the top position of this column. This entr is now a pivot and this row is now a pivot row
    \item Use EROs to change all other entries below the pivot in this pivot column to 0
    \item Consider the submatrix formed by covering up the current pivot row and all previous pivot rows. If there are no more rows or if the only remaining rows are zero rows, we are finished. Otherwise, repeat steps 1 and 2 on the submatrix. Continus in this manner, convering up the current pivot row to obtain a matrix with one less row until remain or we obtain a submatrix with only zero rows 
\end{enumerate}
\subsection{Obtaining the RREF from an REF}
\subsubsection*{Algorithm}
Start with a matrix in REF
\begin{enumerate}
    \item Select the rightmost pivot column. If the pivot is not already 1, use EROs to change it to 1
    \item Use EROs to change all entries above the pivot in this pivot column to 0
    \item Consider the submatrix formed by covering up the current pivot row and all other rows below it. If there are no more rows, then we are finished. Otherwise, repeat steps 1 and 2 on the submatrix until no rows remain
\end{enumerate}
\subsection{Variables in RREF to a Solution}
\subsubsection{Basic Variable, Free Variable}
Consider a system of linear equations. Let $R$ be an REF of the coefficient matrix of this system. 
If the $i^{th}$ column of this matrix contians a pivot, then we call $x_i$ a \textbf{basic variable}.
Otherwise, we call $x_i$ a \textbf{free variable}
\subsection{Characterizing Consistent Systems}
\subsubsection{Notation}
We use $M_{m\times n}(\mathbb{F})$ to denote the set of all $m\times n$ matrices with entries from $\mathbb{F}$, 
or more specifically, sometimes we replace $\mathbb{F}$ with $\mathbb{R}$ or $\mathbb{C}$. If $m=n$ it is common 
to abbreviate $M_{m\times n}(\mathbb{F})$ as $M_n(\mathbb{F})$
\subsubsection{Rank}
Let $A\in M_{m\times n}(\mathbb{F})$ such that RREF(A) has exactly r pivots. Then we say that the rank of A is r, and we write $rank(A) = r$
\subsubsection{Rank Bounds}
If $A\in M_{m\times n}(\mathbb{F})$, then $rank(A)\leq min(m,n)$
\subsubsection{Consistent System Test}
Let A be the coefficient matrix of a system of linear equations and let $[A|\vec{b}]$ be the augmented matrix of the system. 
The system is consistent if and only if $rank(A) = rank([A|\vec{b}])$
\subsection{System Rank Theorem}
Let $A\in M_{m\times n}(\mathbb{F})$ with $rank(A) = r$
\begin{itemize}
    \item[(a)] Let $\vec{b}\in\mathbb{F}^m$. If the system of linear equations with augmented matrix $[A|\vec{b}]$ is consistent, then the solution set to this system will contain $n-r$ parameters 
    \item[(b)] The system with augmented matrix $[A|\vec{b}]$ is consistent for every $\vec{b}\in\mathbb{F}^m$ if and only if $r=m$
\end{itemize}
\subsection{Definitions}
\subsubsection{Nullity}
Let $A\in M_{m\times n}(\mathbb{F})$ with rank$(A)=r$. We define the nullity of A, written nullity(A), to be the integer $n-r$
\subsubsection{Homogeneous and Non-homogeneous Syystems}
We say that a system of linear equations is homogeneous if all the constant terms on the right-hand side of the equations are zero. 
Otherwise we say that the system is non-homogeneous
\subsubsection{Trivial Solution}
For a homogeneous system with variables $x_1, x_2, \cdots, x_n$, the trivial solution 
is the solution $x_1=x_2=\cdots=x_n=0$
\subsubsection{Nullspace}
The solution set of a homogeneous system of linear equations with coefficient matrix A, 
written Null(A), is called the nullspace of A.
\subsection{Matrix-Vector Multiplication}
\subsubsection{Row Vector}
A row vector is a matrix with exactly one row. For a matrix $A\in M_{m\times n}(\mathbb{F})$, we 
will denote the $i^{th}$ row of A by $\vec{row_i}(A)$. That is,
\[\vec{row_i}(A) = [a_{i1}\text{ }a_{i2}\text{ }\cdots\text{ }a_{in}]\]
\subsubsection{Matrix-Vector Multiplication in Terms of the Individual Entries}
Let $A\in M_{m\times n}(\mathbb{F})$ and $\vec{x}\in\mathbb{F}^n$. We define the product
$A\vec{x}$ as follows:
\[A\vec{x} = \begin{bmatrix}
    a_{11} &a_{12} &\cdots &a_{1n} \\
    a_{21} &a_{22} &\cdots &a_{2n} \\
    \vdots &\vdots &\vdots &\vdots \\
    a_{m1} &a_{m2} &\cdots &a_{mn}
\end{bmatrix} \begin{bmatrix}
    x_1 \\x_2 \\\vdots \\x_n
\end{bmatrix} = \begin{bmatrix}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n 
\end{bmatrix}\]
\subsubsection{Linearity of Matrix-Vector Multiplication}
Let $A\in M_{m\times n}(\mathbb{F})$. Let $\vec{x}, \vec{y}\in\mathbb{F}^n$ and $c\in\mathbb{F}$. Then 
\begin{itemize}
    \item[a] $A(\vec{x}+\vec{y}) = A\vec{x} + A\vec{y}$
    \item[b] $A(c\vec{x}) = cA\vec{x}$
\end{itemize}
\subsection{Another Way of Representing Linear Systems of Equations}
\subsubsection*{Example}
The system of equations 
\begin{align*}
    5x_1 +4x_2 -7x_3 +2x_4 &= -1 \\
    -6x_1 -2x_2 +3x_3  &= 2 \\
    +3x_2 +5x_3 +5x_4 &= 7
\end{align*}
can be represented as $A\vec{x} = \vec{b}$, where 
\[A = \begin{bmatrix}
    5 &4 &-7 &2 \\
    -6 &-2 &3 &0 \\
    0 &3 &5 &5 
\end{bmatrix} \text{ and } \vec{b} = \begin{bmatrix} -1 \\2 \\7 \end{bmatrix}\]
\subsection{Solve Homogeneous System}
\subsubsection{Particular Solution}
Let $A\vec{x} = \vec{b}$ be a consistent system of linear equations. We refer to a solution of this system, 
$\vec{x_p}$ as a particular solution to this system 
\subsubsection{Associated Homogeneous System}
Let $A\vec{x} = \vec{b}$, where $\vec{b}\neq \vec{0}$, be a non-homogeneous system of linear equations. 
The associated homogeneous system is the system $A\vec{x} = \vec{0}$
\subsubsection{Solutions to $A\vec{x}=\vec{0} and A\vec{x}=\vec{b}$}
Let $A\vec{x}=\vec{b}$, where $\vec{b}\neq\vec{0}$, be a consistent non-homogeneous system of linear equations with solution set $T$. Let 
$A\vec{x} = \vec{0}$ be the associated homogeneous system with solution set $S$. If $\vec{y}\in T$, then 
$T = \{\vec{w} + \vec{y}: \vec{w}\in S\}$ 

\section{Matrices}
\subsection{Column Space}
Let $A\in M_{m\times n}$, column space of $A$ is $Col(A) = \spn\{\vec{a_1}, \vec{a_2}, \cdots, \vec{a_n}\}$ \\
Let $A\in M_{m\times n} (\mathbb{F})$ and $\vec{b}\in\mathbb{F}^m$. The system of linear equations $A\vec{x}=\vec{b}$ is consistent if and only if $\vec{b}\in Col(A)$
\subsection{Transpose}
Let $A\in M_{m\times n}$. Define $A^T$ by $(A^T)_{ij} = (A)_{ji}$, $A^T\in M_{n\times m}$
\subsection{Row Space}
Let $A\in M_{m\times n} (\mathbb{F})$. We define $Row(A)$ to the span of the transposed rows of $A$. $Row(A) = \spn\{(\vec{row_1}(A))^T, \cdots, (\vec{row_n}(A))^T\}$
\subsection{Column Extraction Lemma}
Let $A = [\vec{a_1}, \vec{a_2}, \cdots, \vec{a_n}] \in M_{m\times n}(\mathbb{F})$. Then $A\vec{e_i} = \vec{a_i}$ for all $i = 1, \cdots, n$
\subsection{Matrix Equality}
Let $A,B\in M_{m\times n}(\mathbb{F})$. Then $A=B$ if and only if $a_{ij} = b_{ij}$ for $1\leq i\leq m$ and $1\leq j\leq n$
\subsection{Equaliity of Matrices}
Let $A,B\in M_{m\times n}(\mathbb{F})$. Then $A=B$ if and only if $A\vec{x} = B\vec{x}$ for all $\vec{x}\in\mathbb{F}^n$
\subsection{Matrix Multiplication}
Let $A\in M_{m\times n} (\mathbb{F})$ and $B\in M_{m\times p} (\mathbb{F})$. We define $AB = C$ to be the matrix $C\in M_{n\times p} (\mathbb{F})$, constructed as:
\[C = AB = A[\vec{b_1}, \vec{b_2}, \cdots, \vec{b_p}] = [A\vec{b_1}, A\vec{b_2}, \cdots, A\vec{b_p}]\]
That is, $j^{th}$ column $C$, $\vec{c_j}$, is obtained by multiplying the matrix $A$ by the $j^{th}$ column of the matrix $B$: 
\[\vec{c_j} = A\vec{b_j} \text{ for } j = 1, \cdots, p\]
\subsection{Matrix Addition}
Let $A,B\in M_{m\times n}(\mathbb{F})$. We define $A+B = C$ to be the matrix $C\in M_{m\times n}(\mathbb{F})$ whose $(i,j)^{th}$ entry is 
\[c_{ij} = a_{ij} + b_{ij} \text{ for all } i = 1, \cdots, m \text{ and } j = 1, \cdots, n\]
\subsection*{Matrix Addition}
If $A,B,C\in M_{m\times n}(\mathbb{F})$
\begin{enumerate}
    \item $A + B = B + A$
    \item $(A + B) + C = A + (B + C) = A + B + C$
\end{enumerate}
\subsection*{Proposition of Matrix Multiplication}
If $A,B\in M_{m\times n}(\mathbb{F})$, $C, D\in M_{n\times p}(\mathbb{F})$ and $E\in M_{p\times q}(\mathbb{F})$
\begin{enumerate}
    \item $(A+B)C = AC + BC$
    \item $A(C+D) = AC + AD$
    \item $(AC)E = A(CE) = ACE$
\end{enumerate}
\subsection{Additive Inverse}
If $A\in M_{m\times n}(\mathbb{F})$. We define the additive inverse of $A$ to be the matrix $-A$ whose $ij^{th}$ entry is $-a_{ij}$ for all $i = 1, \cdots, m$ and $j = 1, \cdots, n$
\subsection{Zero Matrix}
The $m\times n$ zero matrix, all of those entries are 0, is denoted $0_{m\times n}\in M_{m\times n}(\mathbb{F})$
\subsection*{Properties of the Additive Inverse and the Zero Matrix}
If $A\in M_{m\times n}(\mathbb{F})$, $0\in M_{m\times n}(\mathbb{F})$
\begin{enumerate}
    \item $0+A = A+0 = A$
    \item $A + (-A) = (-A) + A = 0$
\end{enumerate}
\subsection{Multiplication of a matrix by a scalar}
Let $A\in M_{m\times n}(\mathbb{F})$ and $c\in\mathbb{F}$. We define the matrix $cA\in M_{m\times n}(\mathbb{F})$
whose $(i,j)^{th}$ entry is $(cA)_{ij} = c(A_{ij}) = ca_{ij}$ for all $i = 1, \cdots, m$ and $j = 1, \cdots, n$
\subsection*{Properties of Multiplication of a Matrix by a Scalar}
If $A,B\in M_{m\times n}(\mathbb{F})$, $C\in M_{n\times k}(\mathbb{F})$, and $r,s\in\mathbb{F}$
\begin{enumerate}
    \item $s(A+B) = sA + sB$
    \item $(r+s)A = rA + sA$
    \item $r(sA) = (rs)A$
    \item $s(AC) = (sA)C = A(sC)$
\end{enumerate}
\subsection{Properties of the Matrix Transpose}
If $A,B\in M_{m\times n}(\mathbb{F})$, $C\in M_{n\times k}(\mathbb{F})$, and $s\in\mathbb{F}$
\begin{enumerate}
    \item $(A+B)^T = A^T + B^T$
    \item $(sA)^T = s(A^T)$
    \item $(AC)^T = C^TA^T$
    \item $(A^T)^T = A$
\end{enumerate}
\subsection{Elementary Matrices}
\subsubsection{Definition}
A matrix that can be obtained by performing a single ERO on the identity matrix is called an elementary matrix
\subsubsection{Proposition}
Let $A\in M_{m\times n}(\mathbb{F})$ and suppose that a single ERO is performed on it to produce matrix B. Suppose, also, 
that we perform the same EOR on the matrix $I_m$ to produce the elementary matrix E. Then \[B = EA\] 
\subsubsection{Corollary}
Let $A\in M_{m\times n}(\mathbb{F})$ and suppose that a finite number of EROs, numbered 1 through k, are performed on A to produce 
a matrix B. Let $E_i$ denote the elementary matrix corresponding to the $i^{th}$ ERO $(1\leq i\leq k)$ applied to $I_m$. Then \[B = E_k\cdots E_2 E_1 A\]
\subsection{Invertibility}
\subsubsection{Invertible Matrix}
We say that an $n\times n$ matrix A is invertible if there exist $n\times n$ matrices B and C such that $AB = CA = I_n$
\subsubsection{Proposition: Equality of Left and Right Inverses}
Let $A\in M_{m\times n}(\mathbb{F})$. If there exists matrices B and C in $M_{n\times n}(\mathbb{F})$ such that $AB = CA = I_n$, then $B=C$
\subsubsection{Theorem: Left Invertible Iff Right Invertible}
For $A\in M_{m\times n}(\mathbb{F})$, there exists an $n\times n$ matrix B such that $AB = I_n$ iff there exists an $n\times n$ matrix C such that $CA = I_n$
\subsubsection{Inverse of a Matrix}
If an $n\times n$ matrix $A$ is invertible, we refer to the matrix $B$ such that $AB = I_n$ as the inverse of $A$. 
We denote the inverse of $A$ by $A^{-1}$. The inverse of $A$ satisfies \[AA^{-1} = A^{-1}A = I_n\]
\subsubsection{Theorem: Invertibility Criteria - First Version} 
Let $A\in M_{n\times n}(\mathbb{F})$. The following three conditions are equivalent:
\begin{enumerate}
    \item[(a)] A is invertible 
    \item[(b)] rank(A) = n 
    \item[(c)] RREF(A) = $I_n$  
\end{enumerate}
\subsection{Algorithm for Checking Invertibility and Finding the Inverse}
The following algorithm allows you to determine whether an $n\times n$ matrix A is invertible, and it is, 
the algorithm will provide the inverse of A 
\begin{enumerate}
    \item Construct a super-augmented matrix $[A|I_n]$
    \item Find the RREF, $[R|B]$, of $[A|I_n]$
    \item If $R\neq I_n$, conclude that A is not invertible. If $R = I_n$, conclude that A is invertible, and that $A^{-1} = B$
\end{enumerate}
\subsubsection{Proposition: Inverse of a $2\times 2$ Matrix}
Let $A = \begin{bmatrix}
    a &b \\ c &d
\end{bmatrix}$. Then A is invertible if and only if $ad-bc\neq 0$. Furthurmore, if $ad - bc\neq 0$, then 
\[A^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
    d &-b \\ -c &a
\end{bmatrix}\]

% Chapter 5
\section{Chapter 5 - Linear Transformation}
\subsection{From Matrices to Functions}
\subsubsection{Function Determined by a Matrix}
Let $A\in M_{m\times n}(\mathbb{F})$. The function determined by the matrix A is the function \[T_A: \mathbb{F}^n \Rightarrow \mathbb{F}^m\]
defined by \[T_A(\vec{x}) = A\vec{x}\]
\subsubsection{Function Determined by a Matrix is Linear}
Let $A\in M_{m\times n}(\mathbb{F})$ and let $T_A$ be the function determined by the matrix A. Then $T_A$ is linear; that is, 
for any $\vec{x}, \vec{y}\in\mathbb{F}^n$ and any $c\in\mathbb{F}$, the following two properties hold: 
\begin{itemize}
    \item $T_A(\vec{x} + \vec{y}) = T_A(\vec{x}) + T_A(\vec{y})$
    \item $T_A(c\vec{x}) = cT_A(\vec{x})$
\end{itemize}
\subsection{Linear Transformation - Definition} 
\subsubsection{Linear Transformation}
We say that the function $T$: $\mathbb{F}^n\Rightarrow\mathbb{F}^m$ is a linear transformation if, for any $\vec{x}, \vec{y}\in\mathbb{F}^n$
and any $c\in\mathbb{F}$, the following two properties hold:
\begin{enumerate}
    \item $T(\vec{x}+\vec{y}) = T(\vec{x}) + T(\vec{y})$ (linearity over addition)
    \item $T(c\vec{x}) = cT(\vec{x})$ (linearity over scalar multiplication)
\end{enumerate}
\begin{itemize}
    \item We will call $T_A$ the linear transformation determined by A 
    \item We refer to $\mathbb{F}^n$ here as the domain of T and $\mathbb{F}^m$ as the codomain of T, as we would for any function 
    \item Sometimes we use the term linear mapping instead
\end{itemize}
\subsubsection{Alternative Characterization of a Linear Transformation}
Let T: $\mathbb{F}^n \Rightarrow \mathbb{F}^m$ be a function. Then T is a linear transformation if and only if for any 
$\vec{x}, \vec{y} \in \mathbb{F}^n$ and any $c\in\mathbb{F}$ \[T(c\vec{x}+\vec{y}) = cT(\vec{x}) + T(\vec{y})\]
\subsubsection{Zero Maps to Zero}
Let T: $\mathbb{F}^n\Rightarrow\mathbb{F}^m$ be a linear transformation. Then \[T(\vec{0}_{\mathbb{F}^n}) = \vec{0}_{\mathbb{F}^m}\]
\subsection{Range}
\subsubsection{Definition}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation. We define the range of $T$, enoted Range(T), to be the set of all outputs of T. That is, 
\[\text{Range(T)} = \{T(\vec{x}): \vec{x}\in\mathbb{F}^n\}\]
The range of T is a subset of $\mathbb{F}^m$
\subsubsection{Range of a Linear Transformation}
Let $A\in M_{m\times n}(\mathbb{F})$, and let $T_A: \mathbb{F}^n\rightarrow\mathbb{F}^m$ be the lienar transformation determined by A. Then
\[\text{Range}(T_A) = \text{Col}(A)\]
\subsection{Onto}
\subsubsection{Definition}
We say that the transformation $T: \mathbb{F}^n\rightarrow\mathbb{F}^m$ is onto (or surjective) if 
Range(T)$=\mathbb{F}^m$
\subsubsection{Onto Criteria}
Let $A\in M_{m\times n}(\mathbb{F})$ and let $T_A$ be the linear transformation determined by the matrix A. The following statements are equivalent: 
\begin{itemize}
    \item[(a)] $T_A$ is onto 
    \item[(b)] Col(A)$=\mathbb{F}^m$
    \item[(c)] rank(A)$=m$
\end{itemize}
\subsection{Kernel}
\subsubsection{Definition}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation. We define the kernel of T, denoted Ker(T),
to be the set of inputs of T whose output is zero. That is 
\[\text{Ker(T)} = \{\vec{x}\in\mathbb{F}^n: T(\vec{x}) = \vec{0}_{\mathbb{F}^m}\}\]
The kernel of T is a subset of $\mathbb{F}^n$
\subsubsection{Kernel of a Linear Transformation}
Let $A\in M_{m\times n}(\mathbb{F})$ and let $T_A: \mathbb{F}^n\rightarrow\mathbb{F}^m$ be the linear transformation determined by A. Then 
\[\text{Ker}(T_A) = \text{Null(A)}\]
\subsection{One-to-One}
\subsubsection{Definition}
We say that the transformation $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ is one-to-one (or injective)
if, whenever $T(\vec{x}) = T(\vec{y})$, then $\vec{x}=\vec{y}$
\subsubsection{One-to-One Test}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation. Then 
\[\text{T is one-to-one if and only if Ker(T)}=\{\vec{0}_{\mathbb{F}^n}\}\]
\subsubsection{One-to-One Criteria}
Let $A\in M_{m\times n}(\mathbb{F})$ and let $T_A$ be the linear transformation determined by the matrix A. The following statements are equivalent 
\begin{enumerate}
    \item $T_A$ is one-to-one 
    \item Null(A) $=\{\vec{0}_{\mathbb{F}^n}\}$
    \item nullity(A) = 0
    \item rank(A) = n
\end{enumerate}
\subsection{Invertibility Criteria (More)}
Let $A\in M_{m\times n}(\mathbb{F})$ be a square matrix and let $T_A$ be the linear transformation determined by the matrix A. The following statements are equivalent 
\begin{enumerate}
    \item A is invertible
    \item $T_A$ is one-to-one 
    \item $T_A$ is onto 
    \item Null(A) = $\{\vec{0}\}$. That is, the only solution to the homogeneous system $A\vec{x} = \vec{0}$ is the trivial solution $\vec{x} = \vec{0}$
    \item Col(A) = $\mathbb{F}^n$. That is, for every $\vec{b}\in\mathbb{F}^n$, the system $A\vec{x} = \vec{b}$ is consistent 
    \item nullity(A) = 0
    \item rank(A) = n 
    \item RREF(A) = $I_n$
\end{enumerate}
\subsection{Standard Matrix}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation. We define the standard matrix of T, denoted by $[T]_\epsilon$, to be $m\times n$ matrix whose columns are the images under T of the vectors in the standard basis of $\mathbb{F}^n$
\begin{align*}
    [T]_\epsilon &= [T(\vec{e_1}) \text{ } T(\vec{e_2}) \text{ } \cdots \text{ } T(\vec{e_n})] \\
                 &= [T(\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}) \text{ } T(\begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}) \text{ } \cdots \text{ } T(\begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix})]
\end{align*}
\subsection*{Remarkable Result}
\subsubsection{Every Linear Transformation is Determined by a matrix}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation and let $[T]_\epsilon$ be the standard matrix of T. Then for all $\vec{x}\in\mathbb{F}^n$
\[T(\vec{x}) = [T]_\epsilon\vec{x}\]
That is, $T = T_{[T]_\epsilon}$ is the linear transformation determined by the matrix $[T]_\epsilon$
\subsubsection{Proposition 5.5.3 (Corollary)}
Let $T:\mathbb{R}\rightarrow\mathbb{R}$ be a linear transformation. Then there is a real number $m\in\mathbb{R}$ such that $T(x) = mx$ for all $x\in\mathbb{R}$
\subsection{Properties of Standard Matrices}
Let $A\in M_{m\times n}(\mathbb{F})$, let $T_A: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be the linear transformation determined by A, and let $T: \mathbb{F}^n \rightarrow\mathbb{F}^m$ be a linear transformation
\begin{enumerate}
    \item $T_{[T]_\epsilon} = T$
    \item $[T_A]_\epsilon = A$
    \item T is onto if and only if rank($[T]_\epsilon$) = m 
    \item T is one-to-one if and only if rank($[T]_\epsilon$) = n 
\end{enumerate}
\subsection{Composition of Linear Transformations}
Let $T_1: \mathbb{F}^n \rightarrow\mathbb{F}^m$ and $T_2: \mathbb{F}^m \rightarrow\mathbb{F}^p$ be linear transformations. We define the function $T_2\circ T_1: \mathbb{F}^n\rightarrow\mathbb{F}^p$ by 
\[(T_2\circ T_1)(\vec{x}) = T_2(T_1(\vec{x}))\]
The function $T_2\circ T_1$ is called the composite function of $T_2$ and $T_1$
\subsection{Result}
\subsubsection{Composition of Linear Transformation is Linear}
Let $T_1: \mathbb{F}^n \rightarrow\mathbb{F}^m$ and $T_2: \mathbb{F}^m \rightarrow\mathbb{F}^p$ be linear transformations. Then $T_2\circ T_1$ is a linear transformation
\subsubsection{The Standard Matrix of a Composition of Linear Transformations}
Let $T_1: \mathbb{F}^n \rightarrow\mathbb{F}^m$ and $T_2: \mathbb{F}^m \rightarrow\mathbb{F}^p$ be linear transformations. Then the standard matrix of $T_2\circ T_1$ is equal to the 
product of standard matrices of $T_2$ and $T_1$. That is
\[[T_2\circ T_1]_\epsilon = [T_2]_\epsilon[T_1]_\epsilon\]
\subsection{Special Cases}
\subsubsection{Identitty Transformation}
The linear transformation $id_n: \mathbb{F}^n\rightarrow\mathbb{F}^n$ such that $id_n(\vec{x}) = \vec{x}$ for all $\vec{x}\in\mathbb{F}^n$ is called the identity transformation
\subsubsection{$T^P$}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^n$ and let $p\geq 0$ be an integer. We recursively define the $p^{th}$ power of T, denoted $T^P$ as follows: 
\[T^0 = id_n \text{ and for } p>0, T^P = T\circ T^{P-1}\]
\subsubsection{Corollary}
Let $T: \mathbb{F}^n \rightarrow\mathbb{F}^n$ be a linear transformation and let $p>1$ be an integer. Then the standard matrix of $T^P$ is the $p^{th}$ power of the standard matrix of T. That is 
\[[T^P]_\epsilon = ([T]_\epsilon)^p\]

% Chapter 6
\section{Chapter 6 - Determinant}
\subsection{Definition of Determinant}
\subsubsection{Determinant of a $1\times1$ and $2\times2$ Matrix}
Let $A=[a_{11}]\in M_{1\times1}(\mathbb{F})$ \\
The determinant of $A$, denoted by det(A), is
\[det(A) = a_{11}\]
Let $A = \begin{bmatrix}a_{11}&a_{12} \\ a_{21}&a_{22}\end{bmatrix}\in M_{2\times2}(\mathbb{F})$ \\
The determinant of $A$ is 
\[det(A) = a_{11}a_{22} - a_{12}a_{21}\]
\subsubsection{$(i,j)^{th}$ Submatrix, $(i,j)^{th}$ Minor}
Let $A\in M_{n\times n}(\mathbb{F})$ \\
The $(i,j)^{th}$ submatrix of A, denoted by $M_{ij}(A)$ is the $(n-1)\times(n-1)$ matrix obtained from A by removing the $i^{th}$ row 
and the $j^{th}$ column from A \\
The determinant of $M_{ij}(A)$ is known as the $(i,j)^{th}$ minor of A 
\subsubsection{Determinant of an $n\times n$ Matrix}
Let $A\in M_{n\times n}(\mathbb{F})$ for $n\geq2$ \\
The determinant function, $det: M_{n\times n}(\mathbb{F})\rightarrow\mathbb{F}$ by 
\[det(A) = \sum_{j=1}^{n}a_{1j}(-1)^{1+j}det(M_{1j}(A))\]
\subsubsection{$i^{th}$ Row Expansion of the Determinant}
Let $A\in M_{n\times n}(\mathbb{F})$ with $n\geq2$ and let $i\in\{1,\cdots,n\}$. Then
\[det(A) = \sum_{j=1}^{n}a_{ij}(-1)^{i+j}det(M_{ij}(A))\]
\subsubsection{$j^{th}$ Column Expansion of the Determinant}
Let $A\in M_{n\times n}(\mathbb{F})$ with $n\geq2$ and let $j\in\{1,\cdots,n\}$. Then
\[det(A) = \sum_{i=1}^{n}a_{ij}(-1)^{i+j}det(M_{ij}(A))\]
\subsubsection{Easy Determinants for a Square Matrix $A\in M_{n\times n}(\mathbb{F})$}
\begin{itemize}
    \item If A has a row consisting only of zeros, then det(A) = 0
    \item If A has a column consisting only of zeros, then det(A) = 0
    \item If $A = \begin{bmatrix} a_{11} &* &\cdots &* \\ 0 &a_{22} &\cdots &* \\ \vdots &\vdots &\ddots &\vdots \\ 0 &0 &\cdots &a_{nn} \end{bmatrix}$ is upper triangular (also works for lower triangular), $det(A) = a_{11}a_{22}\cdots a_{nn}$
    \item $det(I_n) = 1$
    \item Let $A\in M_{n\times n}(\mathbb{F})$. $det(A) = det(A^T)$
\end{itemize}
\subsection{Elementary Row Operations and the Determinant}
\subsubsection*{Theorem}
Let $A\in M_{n\times n}(\mathbb{F})$
\begin{enumerate}
    \item (Row sawp) If B is obtained from A by interchanging two rows, then det(B) = -det(A)
    \item (Row scale) If B is obtained from A by multiplying a row by $m\neq0$, then det(B) = $m$ det(A)
    \item (Row addition) If B is obtained from A by adding a non-zero multiple of one row to another row, then det(B) = det(A)
    \item[] remains true if instances of "row" replaced with "column"
    \item If A has two identical rows (or two identical columns), then det(A) = 0
    \item Determinants of Elementary Matrices \\
          If E is the elementary matrix of the indicated type, then 
          \begin{enumerate}
            \item (Row swap) det(E) = -1
            \item (Row scale by $m\neq0$) det(E) = m 
            \item (Row addition) det(E) = 1
          \end{enumerate}
    \item Determinant after one ERO \\
          Suppose we perform a single ERO on A to produce the matrix B where the corresponding elementary matrix is E. Then det(B) = det(E)det(A)
    \item Determinant after k EROs \\
          Suppose we perform a sequence of k EROs on the matrix A to obtain the matrix B. Suppose that the elementary matrix corresponding too the $i$th ERO is $E_i$, so 
          \[B = E_k\cdots E_2E_1A\]
          Then 
          \[det(B) = det(E_k\cdots E_2E_1A) = det(E_k)\cdots det(E_2)det(E_1)det(A)\]
\end{enumerate}
\subsection{The Determinant and Invertibility}
\begin{enumerate}
    \item Invertible iff the Determinant is Non-Zero \\
          Let $A\in M_{n\times n}(\mathbb{F})$. Then $A$ is invertible if and only if $det(A)\neq 0$
    \item Determinant of a Product \\
          Let $A, B\in M_{n\times n}(\mathbb{F})$. Then $det(AB) = det(A)det(B)$
    \item Corollary \\
          Let $A, B\in M_{n\times n}(\mathbb{F})$. Then $det(AB) = det(BA)$
    \item Determinant of Inverse \\
          Let $A\in M_{n\times n}(\mathbb{F})$ be invertible. Then $det(A^{-1}) = \dfrac{1}{det(A)}$
\end{enumerate}
\subsection{An Expression for $A^{-1}$}
\subsubsection{Cofactor}
Let $A\in M_{n\times n}(\mathbb{F})$. The $(i,j)^{th}$ cofactor of A, denoted by $C_{ij}(A)$ is defined by 
\[C_{ij}(A) = (-1)^{i+j}det(M_{ij}(A))\]
\subsubsection{Adjugate of a Matrix}
Let $A\in M_{n\times n}(\mathbb{F})$. The adjugate of A, denoted by $adj(A)$, is the $n\times n$ matrix whose $(i,j)^{th}$ entry is 
\[(adj(A))_{ij} = C_{ij}(A)\]
The conjugate of A is the transpose of the matrix of cofactors of A 
\subsubsection{Theorem}
Let $A\in M_{n\times n}(\mathbb{F})$. Then 
\[\text{A }adj(A) = adj(A)A = det(A)I_n\]
\subsubsection{Inverse by Adjugate}
Let $A\in M_{n\times n}(\mathbb{F})$. If $det(A)\neq 0$, then 
\[A^{-1} = \dfrac{1}{det(a)}adj(A)\]
\subsubsection{Cramer's Rule}
Let $A\in M_{n\times n}(\mathbb{F})$ and consider the equation $A\vec{x}=\vec{b}$, where $\vec{b}\in\mathbb{F}^n$ and $det(A)\neq 0$ \\
If $B_j$ is constructed from $A$ by replacing the $j^{th}$ column of $A$ by the column vector $\vec{b}$, then the solution $\vec{x}$ to the equation \[A\vec{x}=\vec{b}\]
is given by \[x_j = \frac{det(B_j)}{det(A)},\quad \text{for all $j=1, \cdots, n$}\]
\subsubsection{Determinant and Geometry}
\subsubsection*{Area of Parallelogram}
Let $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$ and $\vec{w} = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix}$ be vectors in $\mathbb{R}^2$ \\
The are of the parallelogram with sides $\vec{v}$ and $\vec{w}$ is 
\[|det(\begin{bmatrix} v_1 &w_1 \\ v_2 &w_2 \end{bmatrix})|\]

% Chapter 7
\section{Chapter 7 - Eigenvalues and Diagonalization}
\subsection{Eigenvector, Eigenvalue}
\subsubsection{Eigenvector, Eigenvalue and Eigenpair}
Let $A\in M_{n\times n}(\mathbb{F})$. A non-zero vector $\vec{x}$ is an eigenvector of A over $\mathbb{F}$ if there 
exists a scalar $\lambda\in\mathbb{F}$ such that \[A\vec{x} = \lambda\vec{x}\]
The scalar $\lambda$ is called an eigenvalue of A over $\mathbb{F}$, and the pair 
$(\lambda, \vec{x})$ is an eigenpair of A over $\mathbb{F}$
\subsubsection{Eigenvalue Equation or Eigenvalue Problem}
Let $A\in M_{n\times n}(\mathbb{F})$. We refer ro the equation \[A\vec{x} = \lambda\vec{x}\quad\text{or}\quad (A-\lambda I)\vec{x} = \vec{0}\]
as the eigenvalue equation for the matrix A over $\mathbb{F}$. It is also sometimes referred to as the eigenvalue problem
\subsubsection{Characteristic Polynomial and Characteristic Equation}
Let $A\in M_{n\times n}(\mathbb{F})$ and $\lambda\in\mathbb{F}$. The characteristic polynomial of A, denoted by $C_A(\lambda)$, is 
\[C_A(\lambda) = det(A-\lambda I)\]
The characteristic equation of A is \[C_A(\lambda) = 0\]
\subsection{Trace}
\subsubsection{Definition}
Let $A\in M_{n\times n}(\mathbb{F})$. We define the trace of A by
\[tr(A) = \sum_{i=1}^{\infty}a_{ii}\]
That is, the trace of a square matrix is the sum of its diagonal entries 
\subsection{Properties of the Characteristic Polynomial}
\subsubsection{Features of the Characteristic Polynomial}
Let $A\in M_{n\times n}(\mathbb{F})$ have characteristic polynomial $C_A(\lambda) = det(A-\lambda I)$. 
Then $C_A(\lambda)$ is a degree n polynomial in $\lambda$ of the form 
\[C_A(\lambda) = c_n\lambda^n + c_{n-1}\lambda^{n-1}+\cdots+c_1\lambda + c_0\]
where 
\begin{enumerate}
    \item $c_n = (-1)^n$
    \item $c_{n-1} = (-1)^{(n-1)}tr(A)$
    \item $c_0 = det(A)$
\end{enumerate}
\subsection{Connecting to Eigenvalues}
\subsubsection{Characteristic Polynomial and Eigenvalues over $\mathbb{C}$}
Let $A\in M_{n\times n}(\mathbb{F})$ have characteristic polynomial
\[C_A(\lambda) = c_n\lambda^n + c_{n-1}\lambda^{n-1}+\cdots+c_1\lambda + c_0\]
and n eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_n$ (possibly repeated) in $\mathbb{C}$. Then 
\begin{enumerate}
    \item $c_{n-1} = (-1)^{(n-1)}\displaystyle\sum_{i=1}^{n}\lambda_i$ 
    \item $c_0 = \prod_{i=1}^{n}\lambda_i$
\end{enumerate}
\subsubsection{Eigenvalues and Trace / Determinant}
Let $A\in M_{n\times n}(\mathbb{F})$ have n eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_n$ (possibly repeated) in $\mathbb{C}$. Then 
\begin{enumerate}
    \item $\displaystyle\sum_{i=1}^{n}\lambda_i = tr(A)$
    \item $\displaystyle\prod_{i=1}^{n}\lambda_i = det(A)$
\end{enumerate}
\subsection{Eigenspaces}
\subsubsection{Linear Combinations of Eigenvectors}
Let $c,d\in\mathbb{F}$ and suppose that $(\lambda_1, \vec{x})$ and $(\lambda_1, \vec{y})$ are 
eigenpairs of a matrix A over $\mathbb{F}$ with the same eigenvalue $\lambda_1$. If $c\vec{x} + d\vec{y}\neq \vec{0}$,
then $(\lambda_1, c\vec{x} + d\vec{y})$ is also an eigenpair for A with eigenvalue $\lambda_1$
\subsubsection{Eigenspace}
Let $A\in M_{n\times n}(\mathbb{F})$ and let $\lambda\in\mathbb{F}$. The eigenspace of A associated with $\lambda$, denoted by $E_\lambda(A)$, 
is the solution set to the system $(A - \lambda I)\vec{x} = \vec{0}$ over $\mathbb{F}$. That is
\[E_\lambda(A) = Null(A-\lambda I)\]
If the choice of A is clear, we abbreviate this as $E_\lambda$
\subsection{Motivating Fact and Definition}
\subsubsection{Fact}
If $D = diag(d_1, \cdots, d_n)\in M_{n\times n}(\mathbb{F})$, then $D^k = diag(d_1^k, \cdots, d_n^k)$ for all $k\in\mathbb{N}$
\subsubsection{Similar}
Let $A, B\in M_{n\times n}(\mathbb{F})$. We say that A is similar to B over $\mathbb{F}$ if there exists an invertible matrix $P\in M_{n\times n}(\mathbb{F})$ such that $P^{-1}AP = D$
We say that the matrix P diagonalizes A 
\subsection{Proposition}
\subsubsection{n Distinctt Eigenvalues $\Rightarrow$ Diagonalizable}
If $A\in M_{n\times n}(\mathbb{F})$ has n distinct eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_n$ in $\mathbb{F}$, then 
A is diagonalizable over $\mathbb{F}$ \\
More specifically, if we let $(\lambda_1, \vec{v_1}), (\lambda_2, \vec{v_2}), \cdots, (\lambda_n, \vec{v_n})$ be 
eigenpairs of A over $\mathbb{F}$, and if we let $P = [\vec{v_1}\vec{v_2}\cdots\vec{v_n}]$ be the 
matrix whose columns are eigenvectors corresponding to the distinct eigenvalues, then 
\begin{enumerate}
    \item P is invertible 
    \item $P^{-1}AP = D = diag(\lambda_1, \lambda_2, \cdots, \lambda_n)$
\end{enumerate}

% Chapter 8
\section{Chapter 8 - Subspaces and Bases}
\subsection{Subspace}
\subsubsection{Definition}
A subset $V$ of $\mathbb{F}^n$ is called a subspace of $\mathbb{F}^n$ if the following properties are all satisfied 
\begin{enumerate}
    \item $\vec{0}\in V$
    \item For all $\vec{x},\vec{y}\in V$, $\vec{x}+\vec{y}\in V$
    \item For all $\vec{x}\in V$ and $c\in\mathbb{F}$, $c\vec{x}\in V$
\end{enumerate}
\subsubsection{Proposition: Examples of Subspaces}
\begin{enumerate}
    \item $\{\vec{0}\}$ and $\mathbb{F}^n$ are subspaces of $\mathbb{F}^n$
    \item If $\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ is a subset of $\mathbb{F}^n$, then $Span\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ is a subspace of $\mathbb{F}^n$
    \item If $A\in M_{m\times n}(\mathbb{F})$, then the solution set to the homogeneous system $A\vec{x}=\vec{0}$ is a subspace of $\mathbb{F}^n$
    \item If $A\in M_{m\times n}(\mathbb{F})$, then $Col(A)$ is a subspace of $\mathbb{F}^m$
    \item If $T:\mathbb{F}^n\rightarrow\mathbb{F}^m$ is a linear transformation, then the range of $T$, $Range(T)$, is a subspace of $\mathbb{F}^m$
    \item If $T:\mathbb{F}^n\rightarrow\mathbb{F}^m$ is a linear transformation, then the kernel of $T$, $Ker(T)$, is a subspace of $\mathbb{F}^n$
    \item If $A\in M_{n\times n}(\mathbb{F})$ and if $\lambda\in\mathbb{F}$, then the eigenspace $E_\lambda$ is a subspace of $\mathbb{F}^n$
\end{enumerate}
\subsubsection{Subspace Test}
Let $V$ be a subset of $\mathbb{F}^n$. Then $V$ is a subspace of $\mathbb{F}^n$ if and only if 
\begin{enumerate}
    \item $V$ is non-empty 
    \item for all $\vec{x},\vec{y}\in V$ and $c\in\mathbb{F}$, $c\vec{x}+\vec{y}\in V$
\end{enumerate}
\subsection{Linear Dependence}
The vectors $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\in\mathbb{F}^n$ are linearly dependent if there exists scalars $c_1, c_2, \cdots, c_k\in\mathbb{F}$, not all zero, such that 
$c_1\vec{v_1}+c_2\vec{v_2}+\cdots+c_k\vec{v_k} = \vec{0}$\\
If $U=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$, then we say thhat the seet $U$ is a linearly dependent set (or simply that $U$ is linearly dependent) to mean that the vectors  
$\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$ are linearly dependent
\subsection{Linear Independence}
The vectors $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\in\mathbb{F}^n$ are linearly independent if there do not exist scalars $c_1, c_2, \cdots, c_k\in\mathbb{F}$, not all zero, such that 
$c_1\vec{v_1}+c_2\vec{v_2}+\cdots+c_k\vec{v_k} = \vec{0}$\\
Equivalently we say that $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\in\mathbb{F}^n$ are linearly independent if the only solution to the equation 
\[c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_k\vec{v_k} = \vec{0}\]
is the trivial solution $c_1 = c_2 = \cdots = c_k = 0$\\
If $U=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$, then we say thhat the seet $U$ is a linearly independent set (or simply that $U$ is linearly independent) to mean that the vectors  
$\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$ are linearly independent
\subsection{Basis}
Let $V$ be a subspace of $\mathbb{F}^n$ and let $\mathcal{B}=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a finite set of vectors contained in $V$. We say 
that $\mathcal{B}$ is a basis for $V$ if 
\begin{enumerate}
    \item $\mathcal{B}$ is linearly independent 
    \item $V = Span(\mathcal{B})$
\end{enumerate}
\subsection{Linear Dependence Proposition}
\subsubsection{Proposition 8.3.1}
\begin{enumerate}
    \item The vectors $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$ are linearly dependent if and only if one of the vectors can be written as a linear combination of some of the other vectors 
    \item The vecotrs $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}$ are linearly independent if and only if 
    \[c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}\quad\text{($c_i\in\mathbb{F}$) implies $c_1=\cdots=c_k=0$}\]
\end{enumerate}
\subsubsection{Proposition 8.3.2}
Let $S\subseteq\mathbb{F}^n$
\begin{enumerate}
    \item If $\vec{0}\in S$, then $S$ is linearly dependent 
    \item If $S=\{\vec{x}\}$ contains only one vector, then $S$ is linearly dependent if and only if $\vec{x} = \vec{0}$
    \item If $S=\{\vec{x},\vec{y}\}$ contains only two vectors, then $S$ is linearly dependent if and only if one of the vectors is a multiple of the other 
\end{enumerate}
\subsection{Pivots and Linear Independence}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a set of $k$ vectors in $\mathbb{F}^n$ \\
Let A = $[\vec{v_1}\quad \vec{v_2}\quad \cdots\quad \vec{v_k}]$ be the $n\times k$ matrix whose columns are the vectors in $S$ \\
Suppose $rank(A) = r$ and $A$ has pivots in columns $q_1, q_2, \cdots, q_r$ \\
Let $U=\{\vec{v_{q1}}, \vec{v_{q2}}, \cdots, \vec{v_{qr}}\}$, the set of columns of $A$ that correspond to the pivot columns labelled above \\
Then 
\begin{enumerate}
    \item $S$ is linearly independent if and only if $r=k$
    \item $U$ is linearly independent 
    \item If $\vec{v}$ is in $S$ but not in $U$ then the set $\{\vec{v_{q1}}, \vec{v_{q2}}, \cdots, \vec{v_{qr}}, \vec{v}\}$ is linearly dependent 
    \item $Span(U) = Span(S)$
\end{enumerate}
\subsubsection{Bound on Number of Linearly Independent Vectors}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a set of $k$ vectors in $\mathbb{F}^n$ \\
If $n<k$, then $S$ is linearly dependent
\subsection{Subspaces and Spanning Sets}
\subsubsection{Every Subspace Has a Spanning Set}
Let $V$ be a subspace of $\mathbb{F}^n$. Then there exist vectors $\vec{v_1}, \cdots,\vec{v_k}\in V$ such that 
\[V=Span\{\vec{v_1}, \cdots,\vec{v_k}\}\]
\subsubsection{Every Subspace Has a Basis}
Let $V$ be a subspace of $\mathbb{F}^n$. Then $V$ has a basis 
\subsubsection{Span of Subset}
Let $V$ be a subspace of $\mathbb{F}^n$ and let $S=\{\vec{v_1}, \cdots,\vec{v_k}\}\subseteq V$. Then $Span(S)\subseteq V$
\subsubsection{Spans $\mathbb{F}^n$ iff rank is $n$}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a set of $k$ vectors in $\mathbb{F}^n$ \\
Let A = $[\vec{v_1}\quad \vec{v_2}\quad \cdots\quad \vec{v_k}]$ be the matrix whose columns are the vectors in $S$. Then 
\[Span(S) = \mathbb{F}^n \text{ if and only if } rank(A) = n\]
\subsubsection{Size of Basis for $\mathbb{F}^n$}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a set of $k$ vectors in $\mathbb{F}^n$. If $S$ is a basis for $\mathbb{F}^n$, then $k=n$
\subsubsection{$n$ Vectors in $\mathbb{F}^n$ Span iff Independent}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n}\}$ be a set of $n$ vectors in $\mathbb{F}^n$. Then S is linearly independent if and only if $Span(S) = \mathbb{F}^n$
\subsubsection{Bass From a Spanning Set of Linearly Independent Set}
Let $S=\{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_k}\}$ be a subset of $\mathbb{F}^n$
\begin{itemize}
    \item[a)] If $Span(S) = \mathbb{F}^n$, then there exists a subset $\mathcal{B}$ of $S$ which is a basis for $\mathbb{F}^n$
    \item[b)] If $Span(S)\neq\mathbb{F}^n$ and $S$ is linearly independent, then therre exist vectors $\vec{v}_{k+1},\cdots, \vec{v}_n$ in $\mathbb{F}^n$ such that $\mathcal{B} = \{\vec{v_1},\cdots,\vec{v_k}, \vec{v}_{k+1},\cdots, \vec{v}_n\}$
\end{itemize}
\subsection{Bases for $Col(A)$ and $Null(A)$}
\subsubsection{Basis for $Col(A)$}
Let $A=[\vec{a_1}\quad\cdots\quad\vec{a_n}]\in M_{m\times n}(\mathbb{F})$ and suppose that $RREF(A)$ has pivots in columns $q_1, \cdots, q_r$, where $r=rank(A)$. Then $\{\vec{a_{q1}},\cdots,\vec{a_{qr}}\}$ is a basis for $Col(A)$
\subsubsection{Basis for $Null(A)$}
Let $A\in M_{m\times n}(\mathbb{F})$ and consider the homogeneous linear system $A\vec{x}=\vec{0}$. 
Suppose that, after applying the Gauss-Jordan Algorithm, we obtain $k$ free parameters so that the solution set to
this system is given by
\[Null(A) = \{t_1\vec{x_1}+\cdots+t_k\vec{x_k}: t_1,\cdots,t_k\in\mathbb{F}\}\]
Here, $k=nullity(A)=n-rank(A)$ and the parameters $t_i$ and the vectors $\vec{x_i}$ for 
$1\leq i\leq k$ are obtained using the method outlined earlier. Then $\{\vec{x_1},\cdots, \vec{x_k}\}$
is a basis for $Null(A)$
\subsection{Dimension}
\subsubsection{Definition}
The number of elements in a basis for a subspace $V$ of $\mathbb{F}^n$ is called the 
dimension of $V$. We denote this number by $dim(V)$
\subsubsection{Well-deinfed Dimension}
Let $V$ be a subspace of $\mathbb{F}^n$. If $\mathcal{B} = \{\vec{v_1},\cdots,\vec{v_k}\}$ and 
$\mathcal{C} = \{\vec{w_1},\cdots,\vec{w_l}\}$ are bases for $V$, then $k=l$
\subsection{Dimension Propositions}
\subsubsection{Bound on Dimension of Subspace}
Let $V$ be a subspace of $\mathbb{F}^n$. Then $dim(V)\leq n$
\subsubsection{Rank and Nullity as Dimensions}
Let $A\in M_{m\times n}(\mathbb{F})$. Then 
\begin{enumerate}
    \item $rank(A)=dim(Col(A))$
    \item $nullity(A)=dim(Null(A))$
\end{enumerate}
\subsubsection{Rank-Nullity Theorem}
Let $A\in M_{m\times n}(\mathbb{F})$. Then 
\begin{align*}
    n &= rank(A) + nullity(A) \\   
      &= dim(Col(A)) + dim(Null(A))
\end{align*}
\subsection{Basis}
\subsubsection{Unique Representation Theorem}
Let $\mathcal{B} = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n}\}$ be a basis for $\mathbb{F}^n$. Then, for every vector 
$\vec{v}\in\mathbb{F}^n$, there exists unique scalars $c_1, c_2,\cdots,c_n\in\mathbb{F}$ such that 
\[\vec{v}=c_1\vec{v_1}+c_2\vec{v_2}+\cdots+c_n\vec{v_n}\]
\subsubsection{Coordinates and Components}
Let $\mathcal{B} = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n}\}$ be a basis for $\mathbb{F}^n$. Let the vector $\vec{v}\in\mathbb{F}^n$
have representation
\[\vec{v}=c_1\vec{v_1}+c_2\vec{v_2}+\cdots+c_n\vec{v_n}=\sum_{i=1}^{n}c_i\vec{v_i}\quad(c_i\in\mathbb{F})\]
We call the scalars $c_1,c_2,\cdots,c_n$ the coordinates (or components) of $\vec{v}$ with respect to $\mathcal{B}$, or the $\mathcal{B}$-coordinates of $\vec{v}$
\subsubsection{Coordinate Vector}
Let $\mathcal{B} = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n}\}$ be an ordered basis for $\mathbb{F}^n$. Let $\vec{v}\in\mathbb{F}^n$ have 
coordinates $c_1,c_2,\cdots,c_n$ with respect to $\mathcal{B}$, where the ordering of the scalars $c_i$ matches the ordering in $\mathcal{B}$, that is 
\[\vec{v} = \sum_{i=1}^{n}c_i\vec{v_i}\]
Then the coordinates vector of $\vec{v}$ with respect to $\mathcal{B}$ (or the $\mathcal{B}$-coordinates vector of $\vec{v}$) is the column vector in $\mathbb{F}^n$
\[[\vec{v}]_\mathcal{B} = \begin{bmatrix} c_1\\c_2\\\vdots\\c_n \end{bmatrix}\]
\subsection{Linearity}
\subsubsection{Linearity of Taking Coordinates}
Let $\mathcal{B}=\{\vec{v_1},\cdots,\vec{v_n}\}$ be an ordered basis for $\mathbb{F}^n$. Then the function 
$[\quad]_\mathcal{B}:\mathbb{F}^n\rightarrow\mathbb{F}^n$ defined by sending $\vec{x}$ to $[\vec{x}]_\mathcal{B}$ is linear: 
\begin{enumerate}
    \item For all $\vec{v}$, $\vec{u}\in V$, $[\vec{v}+\vec{u}]_\mathcal{B} = [\vec{v}]_\mathcal{B} + [\vec{u}]_\mathcal{B}$
    \item For all $\vec{v}\in V$ and $c\in\mathbb{F}$, $[c\vec{v}]_\mathcal{B} = c[\vec{v}]_\mathcal{B}$
\end{enumerate}
\subsection{Change of Basis}
\subsubsection{Change-of-Basis Matrix, Change-of-Coordinate Matrix}
Let $\mathcal{B}=\{\vec{v_1},\cdots,\vec{v_n}\}$ and $\mathcal{C}=\{\vec{w_1},\cdots,\vec{w_n}\}$ be ordered bases for $\mathbb{F}^n$ \\
The \textit{change-of-basis} (or \textit{change-of-coordinates}) matrix from $\mathcal{B}$-coordinates to $\mathcal{C}$-coordinates is the 
$n\times n$ matrix 
\[_\mathcal{C}[I]_\mathcal{B} = [[\vec{v_1}]_\mathcal{C},\cdots,[\vec{v_n}]_\mathcal{C}]\]
whose columns are the $\mathcal{C}$-coordinates of the vectors $\vec{v_i}$ in $\mathcal{B}$
\subsubsection{Changing a Basis}
Let $\mathcal{B}=\{\vec{v_1},\cdots,\vec{v_n}\}$ and $\mathcal{C}=\{\vec{w_1},\cdots,\vec{w_n}\}$ be ordered bases for $\mathbb{F}^n$. 
Then $[\vec{x}]_\mathcal{C} = _\mathcal{C}[I]_\mathcal{B}[\vec{x}]_\mathcal{B}$ and $[\vec{x}]_\mathcal{B} = _\mathcal{B}[I]_\mathcal{C}[\vec{x}]_\mathcal{C}$
for all $\vec{x}\in\mathbb{F}^n$
\subsubsection{Corollary}
Let $\vec{x} = [\vec{x}]_\epsilon$ be a vector in $\mathbb{F}^n$, where $\epsilon$ is the standard basis for $\mathbb{F}^n$. If $\mathcal{C}$ is any ordered basis 
for $\mathbb{F}^n$ then $[\vec{x}]_\mathcal{C} = _\mathcal{C}[I]_\epsilon[\vec{x}]_\epsilon$
\subsubsection{Inverse of Change-of-Basis Matrix}
Let $\mathcal{B}$ and $\mathcal{C}$ be two ordered bases of $\mathbb{F}^n$. Then 
\[_\mathcal{B}[I]_\mathcal{C} {_\mathcal{C}[I]_\mathcal{B}} = I_n \quad\text{and}\quad _\mathcal{C}[I]_\mathcal{B} {_\mathcal{B}[I]_\mathcal{C}} = I_n\]
In other words, $_\mathcal{B}[I]_\mathcal{C} = (_\mathcal{C}[I]_\mathcal{B})^{-1}$ and $_\mathcal{C}[I]_\mathcal{B} = (_\mathcal{B}[I]_\mathcal{C})^{-1}$

\section{Chapter 9 - Diagonalization}
\subsection{Linear Operators}
A linear Transformation $T:\mathbb{F}^n\rightarrow\mathbb{F}^m$ where n = m is called a linear operator 
\[T(\vec{x}) = [T]_\epsilon\vec{x} = [T(\vec{e_1})\cdots T(\vec{e_n})]\vec{x}\]
\subsection{Matrix Representations}
\subsubsection{$\mathcal{B}-Matrix of T$}
Let $T:\mathbb{F}^n\rightarrow\mathbb{F}^n$ be a lienar operator and let $\mathcal{B} = \{\vec{v_1}, \vec{v_2},\cdots, \vec{v_n}\}$
be an ordered basis for $\mathbb{F}^n$. We define the $\mathcal{B}-matrix$ of T to be the matrix $[T]_\mathcal{B}$ constructed as follows:
\[[T]_\epsilon\vec{x} = [[T(\vec{v_1})]_\mathcal{B}\quad [T(\vec{v_2})]_\mathcal{B}\cdots[T(\vec{v_n})]_\mathcal{B}]\]
\subsubsection{Proposition 9.1.3}
Let $T:\mathbb{F}^n\rightarrow\mathbb{F}^n$ be a linear operator and let $\mathcal{B} = \{\vec{v_1}, \vec{v_2},\cdots,\vec{v_n}\}$ be an ordered basis for 
$\mathbb{F}^n$. If $\vec{v}\in\mathbb{F}^n$, then 
\[[T(\vec{v})]_\mathcal{B} = [T]_\mathcal{B}[\vec{v}]_\mathcal{B}\]
\subsection{Definitions Addressing "Repeated" Eigenvalues}
\subsubsection{Algebraic Multiplicity}
Let $\lambda_i$ be an eigenvalue of $A\in M_{n\times n}(\mathbb{F})$. The algebraic multiplicity of $\lambda_i$ denoted by $a_{\lambda_i}$, is the largest positive integer 
such that $(\lambda-\lambda_i)^{a_{\lambda_i}}$ divides the characteristic polynomial $C_A(\lambda)$ \\
In other word, $a_{\lambda_i}$ gives the number of times that $(\lambda-\lambda_i)$ terms occur in the fully factorized form of $C_A(\lambda)$
\subsubsection{Geometric Multiplicity}
Let $\lambda_i$ be an eigenvalue of $A\in M_{n\times n}(\mathbb{F})$. The geometric multiplicity of $\lambda_i$, denoted by $g_{\lambda_i}$, is the dimensioin
of the eigenspace $E_{\lambda_i}$. That is $g_{\lambda_i} = dim(E_{\lambda_i})$
\subsubsection{Geometric and Algebraic Multiplicities}
Let $\lambda_i$ be an eigenvalue of the matrix $A\in M_{n\times n}(\mathbb{F})$. Then 
\[1\leq g_{\lambda_i}\leq a_{\lambda_i}\]
\subsection{Merging all the Eigenspaces}
\subsubsection{Proposition 9.4.9}
Let $A\in M_{n\times n}(\mathbb{F})$ with distinct eigenvalues $\lambda_1, \lambda_2,\cdots,\lambda_k$. If their corresponding eigenspaces, 
$E_{\lambda_1}, E_{\lambda_2},\cdots, E_{\lambda_k}$ have bases $\mathcal{B}_1, \mathcal{B}_2, \cdots, \mathcal{B}_k$, then 
$\mathcal{B} = \mathcal{B}_1\cup\mathcal{B}_2\cup\cdots\cup\mathcal{B}_k$ is linearly independent 
\subsubsection{Theorem 9.4.10}
Let $A\in M_{n\times n}(\mathbb{F})$. Suppose that the complete factorization of the characteristic polynomial of A into irreducible factors over 
$\mathbb{F}$ is given by 
\[C_A(\lambda) = (\lambda-\lambda_1)^{a_{\lambda_1}}\cdots(\lambda-\lambda_k)^{a_{\lambda_k}}h(\lambda)\]
where $\lambda_1,\cdots,\lambda_k$ are all of the distinct eigenvalues of A in $\mathbb{F}$ with corresponding algebraic multiplicities $a_{\lambda_1} \cdots a_{\lambda_k}$
and $h(\lambda)$ is a polynomial in $\lambda$ that is irreducible over $\mathbb{F}$\\
Then A is diagonalizable over $\mathbb{F}$ if and only if $h(\lambda)$ is a constant polynomial and $a_{\lambda_1}=g_{\lambda_i}$, for each $i=1,\cdots,k$

\end{document}